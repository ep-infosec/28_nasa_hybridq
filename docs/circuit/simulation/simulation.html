<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>hybridq.circuit.simulation.simulation API documentation</title>
<meta name="description" content="Author: Salvatore Mandra (salvatore.mandra@nasa.gov) …" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hybridq.circuit.simulation.simulation</code></h1>
</header>
<section id="section-intro">
<p>Author: Salvatore Mandra (salvatore.mandra@nasa.gov)</p>
<p>Copyright © 2021, United States Government, as represented by the Administrator
of the National Aeronautics and Space Administration. All rights reserved.</p>
<p>The HybridQ: A Hybrid Simulator for Quantum Circuits platform is licensed under
the Apache License, Version 2.0 (the "License"); you may not use this file
except in compliance with the License. You may obtain a copy of the License at
<a href="http://www.apache.org/licenses/LICENSE-2.0.">http://www.apache.org/licenses/LICENSE-2.0.</a></p>
<p>Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.</p>
<h2 id="types">Types</h2>
<p><strong><code>Array</code></strong>: <code>numpy.ndarray</code></p>
<p><strong><code>TensorNetwork</code></strong>: <code>quimb.tensor.TensorNetwork</code></p>
<p><strong><code>ContractionInfo</code></strong>: <code>(opt_einsum.contract.PathInfo, cotengra.hyper.HyperOptimizer)</code></p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
Author: Salvatore Mandra (salvatore.mandra@nasa.gov)

Copyright © 2021, United States Government, as represented by the Administrator
of the National Aeronautics and Space Administration. All rights reserved.

The HybridQ: A Hybrid Simulator for Quantum Circuits platform is licensed under
the Apache License, Version 2.0 (the &#34;License&#34;); you may not use this file
except in compliance with the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0.

Unless required by applicable law or agreed to in writing, software distributed
under the License is distributed on an &#34;AS IS&#34; BASIS, WITHOUT WARRANTIES OR
CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

Types
-----
**`Array`**: `numpy.ndarray`

**`TensorNetwork`**: `quimb.tensor.TensorNetwork`

**`ContractionInfo`**: `(opt_einsum.contract.PathInfo, cotengra.hyper.HyperOptimizer)`
&#34;&#34;&#34;

from __future__ import annotations
from os import environ
from os.path import basename

_mpi_env = &#39;_&#39; in environ and basename(environ[&#39;_&#39;]) in [&#39;mpiexec&#39;, &#39;mpirun&#39;]
_detect_mpi = &#39;DISABLE_MPI_AUTODETECT&#39; not in environ and _mpi_env

import ctypes
from warnings import warn
from hybridq.gate import Gate
from hybridq.utils import isintegral
from hybridq.gate import property as pr
from hybridq.circuit import Circuit
from tqdm.auto import tqdm
from typing import TypeVar
from opt_einsum.contract import PathInfo
from opt_einsum import contract, get_symbol
from more_itertools import flatten
from sys import stderr
from time import time
import numpy as np
from hybridq.utils import sort, argsort, aligned
from hybridq.utils.transpose import _swap_core
from hybridq.utils.dot import _dot_core, _to_complex_core, _log2_pack_size
from hybridq.circuit.simulation.utils import prepare_state
import hybridq.circuit.utils as utils

# Define types
Array = TypeVar(&#39;Array&#39;)
TensorNetwork = TypeVar(&#39;TensorNetwork&#39;)
ContractionInfo = TypeVar(&#39;ContractionInfo&#39;)


def simulate(circuit: {Circuit, TensorNetwork},
             initial_state: any = None,
             final_state: any = None,
             optimize: any = &#39;evolution&#39;,
             backend: any = &#39;numpy&#39;,
             complex_type: any = &#39;complex64&#39;,
             tensor_only: bool = False,
             simplify: {bool, dict} = True,
             remove_id_gates: bool = True,
             use_mpi: bool = None,
             atol: float = 1e-8,
             verbose: bool = False,
             **kwargs) -&gt; any:
    &#34;&#34;&#34;
    Frontend to simulate `Circuit` using different optimization models and
    backends.

    Parameters
    ----------
    circuit: {Circuit, TensorNetwork}
        Circuit to simulate.
    initial_state: any, optional
        Initial state to use.
    final_state: any, optional
        Final state to use (only valid for `optimize=&#39;tn&#39;`).
    optimize: any, optional
        Optimization to use. At the moment, HybridQ supports two optimizations:
        `optimize=&#39;evolution&#39;` (equivalent to `optimize=&#39;evolution-hybridq&#39;`)
        and `optimize=&#39;tn&#39;` (equivalent to `optimize=&#39;cotengra&#39;`).
        `optimize=&#39;evolution&#39;` takes an `initial_state` (it can either be a
        string, which is processed using ```prepare_state``` or an `Array`)
        and evolve the quantum state accordingly to `Circuit`. Alternatives are:

        - `optimize=&#39;evolution-hybridq&#39;`: use internal `C++` implementation for
          quantum state evolution that uses vectorization instructions (such as
          AVX instructions for Intel processors). This optimization method is
          best suitable for `CPU`s.
        - `optimize=&#39;evolution-einsum&#39;`: use `einsum` to perform the evolution
          of the quantum state (via `opt_einsum`). It is possible to futher
          specify optimization for `opt_einsum` by using
          `optimize=&#39;evolution-einsum-opt&#39;` where `opt` is one of the available
          optimization in `opt_einsum.contract` (default: `auto`). This
          optimization is best suitable for `GPU`s and `TPU`s (using
          `backend=&#39;jax&#39;`).

        `optimize=&#39;tn&#39;` (or, equivalently, `optimize=&#39;cotengra&#39;`) performs the
        tensor contraction of `Circuit` given an `initial_state` and a
        `final_state` (both must be a `str`). Valid tokens for both
        `initial_state` and `final_state` are:

        - `0`: qubit is set to `0` in the computational basis,
        - `1`: qubit is set to `1` in the computational basis,
        - `+`: qubit is set to `+` state in the computational basis,
        - `-`: qubit is set to `-` state in the computational basis,
        - `.`: qubit is left uncontracted.

        Before the actual contraction, `cotengra` is called to identify an
        optimal contraction. Such contraction is then used to perform the tensor
        contraction.

        If `Circuit` is a `TensorNetwork`, `optimize` must be a
        valid contraction (see `tensor_only` parameter).
    backend: any, optional
        Backend used to perform the simulation. Backend must have `tensordot`,
        `transpose` and `einsum` methods.
    complex_type: any, optional
        Complex type to use for the simulation.
    tensor_only: bool, optional
        If `True` and `optimize=None`, `simulate` will return a
        `TensorNetwork` representing `Circuit`. Otherwise, if
        `optimize=&#39;cotengra&#39;`, `simulate` will return the `tuple`
        ```(TensorNetwork```, ```ContractionInfo)```. ```TensorNetwork``` and
        and ```ContractionInfo``` can be respectively used as values for
        `circuit` and `optimize` to perform the actual contraction.
    simplify: {bool, dict}, optional
        Circuit is simplified before the simulation using
        `circuit.utils.simplify`. If non-empty `dict` is provided, `simplify`
        is passed as arguments for `circuit.utils.simplity`.
    remove_id_gates: bool, optional
        Identity gates are removed before to perform the simulation.
        If `False`, identity gates are kept during the simulation.
    use_mpi: bool, optional
        Use `MPI` if available. Unless `use_mpi=False`, `MPI` will be used if
        detected (for instance, if `mpiexec` is used to called HybridQ). If
        `use_mpi=True`, force the use of `MPI` (in case `MPI` is not
        automatically detected).
    atol: float, optional
        Use `atol` as absolute tollerance.
    verbose: bool, optional
        Verbose output.

    Returns
    -------
    Output of `simulate` depends on the chosen parameters.

    Other Parameters
    ----------------
    parallel: int (default: False)
        Parallelize simulation (where possible). If `True`, the number of
        available cpus is used. Otherwise, a `parallel` number of threads is
        used.
    compress: {int, dict} (default: auto)
        Select level of compression for ```circuit.utils.compress```, which is
        run on `Circuit` prior to perform the simulation. If non-empty `dict`
        is provided, `compress` is passed as arguments for
        `circuit.utils.compress`. If `optimize=evolution`, `compress` is set to
        `4` by default. Otherwise, if `optimize=tn`, `compress` is set to `2`
        by default.
    allow_sampling: bool (default: False)
        If `True`, `Gate`s that provide the method `sample` will not be sampled.
    sampling_seed: int (default: None)
        If provided, `numpy.random` state will be saved before sampling and
        `sampling_seed` will be used to sample `Gate`s. `numpy.random` state will
        be restored after sampling.
    block_until_ready: bool (default: True)
        When `backend=&#39;jax&#39;`, wait till the results are ready before returning.
    return_numpy_array: bool (default: True)
        When `optimize=&#39;hybridq&#39;` and `return_numpy_array` is `False, a `tuple`
        of two `np.ndarray` is returned, corresponding to the real and
        imaginary part of the quantu state. If `True`, the real and imaginary
        part are copied to a single `np.ndarray` of complex numbers.
    return_info: bool (default: False)
        Return extra information collected during the simulation.
    simplify_tn: str (default: &#39;RC&#39;)
        Simplification to apply to `TensorNetwork`. Available simplifications as
        specified in `quimb.tensor.TensorNetwork.full_simplify`.
    max_largest_intermediate: int (default: 2**26)
        Largest intermediate which is allowed during simulation. If
        `optimize=&#39;evolution&#39;`, `simulate` will raise an error if the
        largest intermediate is larger than `max_largest_intermediate`.  If
        `optimize=&#39;tn&#39;`, slicing will be applied to fit the contraction in
        memory.
    target_largest_intermediate: int (default: 0)
        Stop `cotengra` if a contraction having the largest intermediate smaller
        than `target_largest_intermediate` is found.
    max_iterations: int (default: 1)
        Number of `cotengra` iterations to find optimal contration.
    max_time: int (default: 120)
        Maximum number of seconds allowed to `cotengra` to find optimal
        contraction for each iteration.
    max_repeats: int (default: 16)
        Number of `cotengra` steps to find optimal contraction for each
        iteration.
    temperatures: list[float] (default: [1.0, 0.1, 0.01])
        Temperatures used by `cotengra` to find optimal slicing of the tensor
        network.
    max_n_slices: int (default: None)
        If specified, `simulate` will raise an error if the number of
        slices to fit the tensor contraction in memory is larger than
        `max_n_slices`.
    minimize: str (default: &#39;combo&#39;)
        Cost function to minimize while looking for the best contraction (see
        `cotengra` for more information).
    methods: list[str] (default: [&#39;kahypar&#39;, &#39;greedy&#39;])
        Heuristics used by `cotengra` to find optimal contraction.
    cotengra: dict[any, any] (default: {})
        Extra parameters to pass to `cotengra`.
    &#34;&#34;&#34;

    # Set defaults
    kwargs.setdefault(&#39;allow_sampling&#39;, False)
    kwargs.setdefault(&#39;sampling_seed&#39;, None)

    # Convert simplify
    simplify = simplify if isinstance(simplify, bool) else dict(simplify)

    # Checks
    if tensor_only and type(optimize) == str and &#39;evolution&#39; in optimize:
        raise ValueError(
            f&#34;&#39;tensor_only&#39; is not support for optimize={optimize}&#34;)

    # Try to convert to circuit
    try:
        circuit = Circuit(circuit)
    except:
        pass

    # Simplify circuit
    if isinstance(circuit, Circuit):
        # Flatten circuit
        circuit = utils.flatten(circuit)

        # If &#39;sampling_seed&#39; is provided, use it
        if kwargs[&#39;sampling_seed&#39;] is not None:
            # Store numpy.random state
            __state = np.random.get_state()

            # Set seed
            np.random.seed(int(kwargs[&#39;sampling_seed&#39;]))

        # If stochastic gates are present, randomly sample from them
        circuit = Circuit(g.sample() if isinstance(g, pr.StochasticGate) and
                          kwargs[&#39;allow_sampling&#39;] else g for g in circuit)

        # Restore numpy.random state
        if kwargs[&#39;sampling_seed&#39;] is not None:
            np.random.set_state(__state)

        # Get qubits
        qubits = circuit.all_qubits()
        n_qubits = len(qubits)

        # Prepare state
        def _prepare_state(state):
            if isinstance(state, str):
                if len(state) == 1:
                    state *= n_qubits
                if len(state) != n_qubits:
                    raise ValueError(
                        &#34;Wrong number of qubits for initial/final state.&#34;)
                return state
            else:
                # Convert to np.ndarray
                state = np.asarray(state)
                # For now, it only supports &#34;qubits&#34; ...
                if any(x != 2 for x in state.shape):
                    raise ValueError(
                        &#34;Only qubits of dimension 2 are supported.&#34;)
                # Check number of qubits is consistent
                if state.ndim != n_qubits:
                    raise ValueError(
                        &#34;Wrong number of qubits for initial/final state.&#34;)
                return state

        # Prepare initial/final state
        initial_state = None if initial_state is None else _prepare_state(
            initial_state)
        final_state = None if final_state is None else _prepare_state(
            final_state)

        # Strip Gate(&#39;I&#39;)
        if remove_id_gates:
            circuit = Circuit(gate for gate in circuit if gate.name != &#39;I&#39;)
        # Simplify circuit
        if simplify:
            circuit = utils.simplify(
                circuit,
                remove_id_gates=remove_id_gates,
                atol=atol,
                verbose=verbose,
                **(simplify if isinstance(simplify, dict) else {}))

        # Stop if qubits have changed
        if circuit.all_qubits() != qubits:
            raise ValueError(
                &#34;Active qubits have changed after simplification. Forcing stop.&#34;
            )

    # Simulate
    if type(optimize) == str and &#39;evolution&#39; in optimize:

        # Set default parameters
        optimize = &#39;-&#39;.join(optimize.split(&#39;-&#39;)[1:])
        if not optimize:
            optimize = &#39;hybridq&#39;
        kwargs.setdefault(&#39;compress&#39;, 4)
        kwargs.setdefault(&#39;max_largest_intermediate&#39;, 2**26)
        kwargs.setdefault(&#39;return_info&#39;, False)
        kwargs.setdefault(&#39;block_until_ready&#39;, True)
        kwargs.setdefault(&#39;return_numpy_array&#39;, True)

        return _simulate_evolution(circuit, initial_state, final_state,
                                   optimize, backend, complex_type, verbose,
                                   **kwargs)
    else:

        # Set default parameters
        kwargs.setdefault(&#39;compress&#39;, 2)
        kwargs.setdefault(&#39;simplify_tn&#39;, &#39;RC&#39;)
        kwargs.setdefault(&#39;max_iterations&#39;, 1)
        try:
            import kahypar as __kahypar__
            kwargs.setdefault(&#39;methods&#39;, [&#39;kahypar&#39;, &#39;greedy&#39;])
        except ModuleNotFoundError:
            warn(&#34;Cannot find module kahypar. Remove it from defaults.&#34;)
            kwargs.setdefault(&#39;methods&#39;, [&#39;greedy&#39;])
        except ImportError:
            warn(&#34;Cannot import module kahypar. Remove it from defaults.&#34;)
            kwargs.setdefault(&#39;methods&#39;, [&#39;greedy&#39;])
        kwargs.setdefault(&#39;max_time&#39;, 120)
        kwargs.setdefault(&#39;max_repeats&#39;, 16)
        kwargs.setdefault(&#39;minimize&#39;, &#39;combo&#39;)
        kwargs.setdefault(&#39;target_largest_intermediate&#39;, 0)
        kwargs.setdefault(&#39;max_largest_intermediate&#39;, 2**26)
        kwargs.setdefault(&#39;temperatures&#39;, [1.0, 0.1, 0.01])
        kwargs.setdefault(&#39;parallel&#39;, None)
        kwargs.setdefault(&#39;cotengra&#39;, {})
        kwargs.setdefault(&#39;max_n_slices&#39;, None)
        kwargs.setdefault(&#39;return_info&#39;, False)

        # If use_mpi==False, force the non-use of MPI
        if not (use_mpi == False) and (use_mpi or _detect_mpi):

            # Warn that MPI is used because detected
            if not use_mpi:
                warn(&#34;MPI has been detected. Using MPI.&#34;)

            from hybridq.circuit.simulation.simulation_mpi import _simulate_tn_mpi
            return _simulate_tn_mpi(circuit, initial_state, final_state,
                                    optimize, backend, complex_type,
                                    tensor_only, verbose, **kwargs)
        else:

            if _detect_mpi and use_mpi == False:
                warn(
                    &#34;MPI has been detected but use_mpi==False. Not using MPI as requested.&#34;
                )

            return _simulate_tn(circuit, initial_state, final_state, optimize,
                                backend, complex_type, tensor_only, verbose,
                                **kwargs)


def _simulate_evolution(circuit: iter[Gate], initial_state: any,
                        final_state: any, optimize: any, backend: any,
                        complex_type: any, verbose: bool, **kwargs):
    &#34;&#34;&#34;
    Perform simulation of the circuit by using the direct evolution of the quantum state.
    &#34;&#34;&#34;

    if _detect_mpi:
        warn(&#34;Detected MPI but optimize=&#39;evolution&#39; does not support MPI.&#34;)

    # Initialize info
    _sim_info = {}

    # Convert iterable to circuit
    circuit = Circuit(circuit)

    # Get number of qubits
    qubits = circuit.all_qubits()
    n_qubits = len(qubits)

    # Check if core libraries have been loaded properly
    if any(not x
           for x in [_swap_core, _dot_core, _to_complex_core, _log2_pack_size]):
        warn(&#34;Cannot find C++ HybridQ core. &#34;
             &#34;Falling back to optimize=&#39;evolution-einsum&#39; instead.&#34;)
        optimize = &#39;einsum&#39;

    # If the system is too small, fallback to einsum
    if optimize == &#39;hybridq&#39; and n_qubits &lt;= max(10, _log2_pack_size):
        warn(&#34;The system is too small to use optimize=&#39;evolution-hybridq&#39;. &#34;
             &#34;Falling back to optimize=&#39;evolution-einsum&#39;&#34;)
        optimize = &#39;einsum&#39;

    if verbose:
        print(f&#39;# Optimization: {optimize}&#39;, file=stderr)

    # Check memory
    if 2**n_qubits &gt; kwargs[&#39;max_largest_intermediate&#39;]:
        raise MemoryError(
            &#34;Memory for the given number of qubits exceeds the &#39;max_largest_intermediate&#39;.&#34;
        )

    # If final_state is specified, warn user
    if final_state is not None:
        warn(
            f&#34;&#39;final_state&#39; cannot be specified in optimize=&#39;{optimize}&#39;. Ignoring &#39;final_state&#39;.&#34;
        )

    # Initial state must be provided
    if initial_state is None:
        raise ValueError(
            &#34;&#39;initial_state&#39; must be specified for optimize=&#39;evolution&#39;.&#34;)

    # Convert complex_type to np.dtype
    complex_type = np.dtype(complex_type)

    # Print info
    if verbose:
        print(f&#34;Compress circuit (max_n_qubits={kwargs[&#39;compress&#39;]}): &#34;,
              end=&#39;&#39;,
              file=stderr)
        _time = time()

    # Compress circuit
    circuit = utils.compress(
        circuit,
        kwargs[&#39;compress&#39;][&#39;max_n_qubits&#39;] if isinstance(
            kwargs[&#39;compress&#39;], dict) else kwargs[&#39;compress&#39;],
        verbose=verbose,
        skip_compression=[pr.FunctionalGate],
        **({k: v for k, v in kwargs[&#39;compress&#39;].items() if k != &#39;max_n_qubits&#39;}
           if isinstance(kwargs[&#39;compress&#39;], dict) else {}))

    # Check that FunctionalGate&#39;s are not compressed
    assert (all(not isinstance(g, pr.FunctionalGate) if len(x) &gt; 1 else True
                for x in circuit
                for g in x))

    # Compress everything which is not a FunctionalGate
    circuit = Circuit(g for c in (c if any(
        isinstance(g, pr.FunctionalGate)
        for g in c) else [utils.to_matrix_gate(c, complex_type=complex_type)]
                                  for c in circuit) for g in c)

    # Get state
    initial_state = prepare_state(initial_state,
                                  complex_type=complex_type) if isinstance(
                                      initial_state, str) else initial_state

    if verbose:
        print(f&#34;Done! ({time()-_time:1.2f}s)&#34;, file=stderr)

    if optimize == &#39;hybridq&#39;:

        if complex_type not in [&#39;complex64&#39;, &#39;complex128&#39;]:
            warn(
                &#34;optimize=evolution-hybridq only support [&#39;complex64&#39;, &#39;complex128&#39;]. Using &#39;complex64&#39;.&#34;
            )
            complex_type = np.dtype(&#39;complex64&#39;)

        # Get float_type
        float_type = np.real(np.array(1, dtype=complex_type)).dtype

        # Get C float_type
        c_float_type = {
            np.dtype(&#39;float32&#39;): ctypes.c_float,
            np.dtype(&#39;float64&#39;): ctypes.c_double
        }[float_type]

        # Load libraries
        _apply_U = _dot_core[float_type]

        # Get swap core
        _swap = _swap_core[float_type]

        # Get to_complex core
        _to_complex = _to_complex_core[complex_type]

        # Get states
        _psi = aligned.empty(shape=(2,) + initial_state.shape,
                             dtype=float_type,
                             order=&#39;C&#39;,
                             alignment=32)
        # Split in real and imaginary part
        _psi_re = _psi[0]
        _psi_im = _psi[1]

        # Check alignment
        assert (_psi_re.ctypes.data % 32 == 0)
        assert (_psi_im.ctypes.data % 32 == 0)

        # Get C-pointers
        _psi_re_ptr = _psi_re.ctypes.data_as(ctypes.POINTER(c_float_type))
        _psi_im_ptr = _psi_im.ctypes.data_as(ctypes.POINTER(c_float_type))

        # Initialize
        np.copyto(_psi_re, np.real(initial_state))
        np.copyto(_psi_im, np.imag(initial_state))

        # Create index maps
        _map = {q: n_qubits - x - 1 for x, q in enumerate(qubits)}
        _inv_map = [q for q, _ in sort(_map.items(), key=lambda x: x[1])]

        # Set largest swap_size
        _max_swap_size = 0

        # Start clock
        _ini_time = time()

        # Apply all gates
        for gate in tqdm(circuit, disable=not verbose):

            # FunctionalGate
            if isinstance(gate, pr.FunctionalGate):
                # Get order
                order = tuple(
                    q
                    for q, _ in sorted(_map.items(), key=lambda x: x[1])[::-1])

                # Apply gate to state
                new_psi, new_order = gate.apply(psi=_psi, order=order)

                # Copy back if needed
                if new_psi is not _psi:
                    # Align if needed
                    _psi = aligned.asarray(new_psi,
                                           order=&#39;C&#39;,
                                           alignment=32,
                                           dtype=_psi.dtype)

                    # Redefine real and imaginary part
                    _psi_re = _psi[0]
                    _psi_im = _psi[1]

                    # Get C-pointers
                    _psi_re_ptr = _psi_re.ctypes.data_as(
                        ctypes.POINTER(c_float_type))
                    _psi_im_ptr = _psi_im.ctypes.data_as(
                        ctypes.POINTER(c_float_type))

                # This can be eventually fixed ...
                if any(x != y for x, y in zip(order, new_order)):
                    raise RuntimeError(&#34;&#39;order&#39; has changed.&#34;)

            elif gate.provides([&#39;qubits&#39;, &#39;matrix&#39;]):

                # Check if any qubits is withing the pack_size
                if any(q in _inv_map[:_log2_pack_size] for q in gate.qubits):

                    #@@@ Alternative way to always use the smallest swap size
                    #@@@
                    #@@@ # Get positions
                    #@@@ _pos = np.fromiter((_map[q] for q in gate.qubits),
                    #@@@                    dtype=int)

                    #@@@ # Get smallest swap size
                    #@@@ _swap_size = 0 if np.all(_pos &gt;= _log2_pack_size) else next(
                    #@@@     k
                    #@@@     for k in range(_log2_pack_size, 2 *
                    #@@@                    max(len(_pos), _log2_pack_size) + 1)
                    #@@@     if sum(_pos &lt; k) &lt;= k - _log2_pack_size)

                    #@@@ # Get new order
                    #@@@ _order = [
                    #@@@     x for x, q in enumerate(_inv_map[:_swap_size])
                    #@@@     if q not in gate.qubits
                    #@@@ ]
                    #@@@ _order += [
                    #@@@     x for x, q in enumerate(_inv_map[:_swap_size])
                    #@@@     if q in gate.qubits
                    #@@@ ]

                    if len(gate.qubits) &lt;= 4:

                        # Get new order
                        _order = [
                            x for x, q in enumerate(_inv_map[:8])
                            if q not in gate.qubits
                        ]
                        _order += [
                            x for x, q in enumerate(_inv_map[:8])
                            if q in gate.qubits
                        ]

                    else:

                        # Get qubit indexes for gate
                        _gate_idxs = [_inv_map.index(q) for q in gate.qubits]

                        # Get new order
                        _order = [
                            x for x in range(n_qubits) if x not in _gate_idxs
                        ][:_log2_pack_size]
                        _order += [x for x in _gate_idxs if x &lt; max(_order)]

                    # Get swap size
                    _swap_size = len(_order)

                    # Update max swap size
                    if _swap_size &gt; _max_swap_size:
                        _max_swap_size = _swap_size

                    # Update maps
                    _inv_map[:_swap_size] = [
                        _inv_map[:_swap_size][x] for x in _order
                    ]
                    _map.update(
                        {q: x for x, q in enumerate(_inv_map[:_swap_size])})

                    # Apply swap
                    _order = np.array(_order, dtype=&#39;uint32&#39;)
                    _swap(
                        _psi_re_ptr,
                        _order.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)),
                        n_qubits, len(_order))
                    _swap(
                        _psi_im_ptr,
                        _order.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)),
                        n_qubits, len(_order))

                # Get positions
                _pos = np.array([_map[q] for q in reversed(gate.qubits)],
                                dtype=&#39;uint32&#39;)

                # Get matrix
                _U = np.asarray(gate.matrix(), dtype=complex_type, order=&#39;C&#39;)

                # Apply matrix
                if _apply_U(
                        _psi_re_ptr, _psi_im_ptr,
                        _U.ctypes.data_as(ctypes.POINTER(c_float_type)),
                        _pos.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)),
                        n_qubits, len(_pos)):

                    raise RuntimeError(&#39;something went wrong&#39;)

            else:
                raise RuntimeError(f&#34;&#39;{gate}&#39; not supported&#34;)

        # Check maps are still consistent
        assert (all(_inv_map[_map[q]] == q for q in _map))

        # Swap back to the correct order
        _order = np.array([_inv_map.index(q) for q in reversed(qubits)
                          ][:_max_swap_size],
                          dtype=&#39;uint32&#39;)
        _swap(_psi_re_ptr,
              _order.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)), n_qubits,
              len(_order))
        _swap(_psi_im_ptr,
              _order.ctypes.data_as(ctypes.POINTER(ctypes.c_uint32)), n_qubits,
              len(_order))

        # Stop clock
        _end_time = time()

        # Copy the results
        if kwargs[&#39;return_numpy_array&#39;]:
            _complex_psi = np.empty(_psi.shape[1:], dtype=complex_type)
            _to_complex(
                _psi_re_ptr, _psi_im_ptr,
                _complex_psi.ctypes.data_as(ctypes.POINTER(c_float_type)),
                2**n_qubits)
            _psi = _complex_psi

        # Update info
        _sim_info[&#39;runtime (s)&#39;] = _end_time - _ini_time

    elif optimize.split(&#39;-&#39;)[0] == &#39;einsum&#39;:

        optimize = &#39;-&#39;.join(optimize.split(&#39;-&#39;)[1:])
        if not optimize:
            optimize = &#39;auto&#39;

        # Split circuits to separate FunctionalGate&#39;s
        circuit = utils.compress(
            circuit,
            max_n_qubits=len(qubits),
            skip_compression=[pr.FunctionalGate],
            **({
                k: v
                for k, v in kwargs[&#39;compress&#39;].items()
                if k != &#39;max_n_qubits&#39;
            } if isinstance(kwargs[&#39;compress&#39;], dict) else {}))

        # Check that FunctionalGate&#39;s are not compressed
        assert (all(not isinstance(g, pr.FunctionalGate) if len(x) &gt; 1 else True
                    for x in circuit
                    for g in x))

        # Prepare initial_state
        _psi = initial_state

        # Initialize time
        _ini_time = time()
        for circuit in circuit:

            # Check
            assert (all(not isinstance(g, pr.FunctionalGate) for g in circuit)
                    or len(circuit) == 1)

            # Apply gate if functional
            if len(circuit) == 1 and isinstance(circuit[0], pr.FunctionalGate):

                # Apply gate to state
                _psi, qubits = circuit[0].apply(psi=_psi, order=qubits)

            else:
                # Get gates and corresponding qubits
                _qubits, _gates = zip(
                    *((c.qubits,
                       np.reshape(c.matrix().astype(complex_type), (2,) *
                                  (2 * len(c.qubits)))) for c in circuit))

                # Initialize map
                _map = {q: get_symbol(x) for x, q in enumerate(qubits)}
                _count = n_qubits
                _path = &#39;&#39;.join((_map[q] for q in qubits))

                # Generate map
                for _qs in _qubits:

                    # Initialize local paths
                    _path_in = _path_out = &#39;&#39;

                    # Add incoming legs
                    for _q in _qs:
                        _path_in += _map[_q]

                    # Add outcoming legs
                    for _q in _qs:
                        _map[_q] = get_symbol(_count)
                        _count += 1
                        _path_out += _map[_q]

                    # Update path
                    _path = _path_out + _path_in + &#39;,&#39; + _path

                # Make sure that qubits order is preserved
                _path += &#39;-&gt;&#39; + &#39;&#39;.join([_map[q] for q in qubits])

                # Contracts
                _psi = contract(_path,
                                *reversed(_gates),
                                _psi,
                                backend=backend,
                                optimize=optimize)

                # Block JAX until result is ready (for a more precise runtime)
                if backend == &#39;jax&#39; and kwargs[&#39;block_until_ready&#39;]:
                    _psi.block_until_ready()

        # Stop time
        _end_time = time()

        # Update info
        _sim_info[&#39;runtime (s)&#39;] = _end_time - _ini_time

    else:

        raise ValueError(f&#34;optimize=&#39;{optimize}&#39; not implemented.&#34;)

    if verbose:
        print(f&#39;# Runtime (s): {_sim_info[&#34;runtime (s)&#34;]:1.2f}&#39;, file=stderr)

    # Return state
    if kwargs[&#39;return_info&#39;]:
        return _psi, _sim_info
    else:
        return _psi


def _simulate_tn(circuit: any, initial_state: any, final_state: any,
                 optimize: any, backend: any, complex_type: any,
                 tensor_only: bool, verbose: bool, **kwargs):
    import quimb.tensor as tn
    import cotengra as ctg

    # Get random leaves_prefix
    leaves_prefix = &#39;&#39;.join(
        np.random.choice(list(&#39;abcdefghijklmnopqrstuvwxyz&#39;), size=20))

    # Initialize info
    _sim_info = {}

    # Alias for tn
    if optimize == &#39;tn&#39;:
        optimize = &#39;cotengra&#39;

    if isinstance(circuit, Circuit):

        # Get number of qubits
        qubits = circuit.all_qubits()
        n_qubits = len(qubits)

        # If initial/final state is None, set to all .&#39;s
        initial_state = &#39;.&#39; * n_qubits if initial_state is None else initial_state
        final_state = &#39;.&#39; * n_qubits if final_state is None else final_state

        # Initial and final states must be valid strings
        for state, sname in [(initial_state, &#39;initial_state&#39;),
                             (final_state, &#39;final_state&#39;)]:
            # Get alphabet
            from string import ascii_letters

            # Check if string
            if not isinstance(state, str):
                raise ValueError(f&#34;&#39;{sname}&#39; must be a valid string.&#34;)

            # Deprecated error
            if any(x in &#39;xX&#39; for x in state):
                from hybridq.utils import DeprecationWarning
                from warnings import warn

                # Warn the user that &#39;.&#39; is used to represent open qubits
                warn(
                    &#34;Since &#39;0.6.3&#39;, letters in the alphabet are used to &#34;
                    &#34;trace selected qubits (including &#39;x&#39; and &#39;X&#39;). &#34;
                    &#34;Instead, &#39;.&#39; is used to represent an open qubit.&#34;,
                    DeprecationWarning)

            # Check only valid symbols are present
            if set(state).difference(&#39;01+-.&#39; + ascii_letters):
                raise ValueError(f&#34;&#39;{sname}&#39; contains invalid symbols.&#34;)

            # Check number of qubits
            if len(state) != n_qubits:
                raise ValueError(f&#34;&#39;{sname}&#39; has the wrong number of qubits &#34;
                                 f&#34;(expected {n_qubits}, got {len(state)})&#34;)

        # Check memory
        if 2**(initial_state.count(&#39;.&#39;) +
               final_state.count(&#39;.&#39;)) &gt; kwargs[&#39;max_largest_intermediate&#39;]:
            raise MemoryError(&#34;Memory for the given number of open qubits &#34;
                              &#34;exceeds the &#39;max_largest_intermediate&#39;.&#34;)

        # Compress circuit
        if kwargs[&#39;compress&#39;]:
            if verbose:
                print(f&#34;Compress circuit (max_n_qubits={kwargs[&#39;compress&#39;]}): &#34;,
                      end=&#39;&#39;,
                      file=stderr)
                _time = time()

            circuit = utils.compress(
                circuit,
                kwargs[&#39;compress&#39;][&#39;max_n_qubits&#39;] if isinstance(
                    kwargs[&#39;compress&#39;], dict) else kwargs[&#39;compress&#39;],
                verbose=verbose,
                **({
                    k: v
                    for k, v in kwargs[&#39;compress&#39;].items()
                    if k != &#39;max_n_qubits&#39;
                } if isinstance(kwargs[&#39;compress&#39;], dict) else {}))

            circuit = Circuit(
                utils.to_matrix_gate(c, complex_type=complex_type)
                for c in circuit)
            if verbose:
                print(f&#34;Done! ({time()-_time:1.2f}s)&#34;, file=stderr)

        # Get tensor network representation of circuit
        tensor, tn_qubits_map = utils.to_tn(circuit,
                                            return_qubits_map=True,
                                            leaves_prefix=leaves_prefix)

        # Define basic MPS
        _mps = {
            &#39;0&#39;: np.array([1, 0]),
            &#39;1&#39;: np.array([0, 1]),
            &#39;+&#39;: np.array([1, 1]) / np.sqrt(2),
            &#39;-&#39;: np.array([1, -1]) / np.sqrt(2)
        }

        # Attach initial/final state
        for state, ext in [(initial_state, &#39;i&#39;), (final_state, &#39;f&#39;)]:
            for s, q in ((s, q) for s, q in zip(state, qubits) if s in _mps):
                inds = [f&#39;{leaves_prefix}_{tn_qubits_map[q]}_{ext}&#39;]
                tensor &amp;= tn.Tensor(_mps[s], inds=inds, tags=inds)

        # For each unique letter, apply trace
        for x in set(initial_state + final_state).difference(&#39;&#39;.join(_mps) +
                                                             &#39;.&#39;):
            # Get indexes
            inds = [
                f&#39;{leaves_prefix}_{tn_qubits_map[q]}_i&#39;
                for s, q in zip(initial_state, qubits)
                if s == x
            ]
            inds += [
                f&#39;{leaves_prefix}_{tn_qubits_map[q]}_f&#39;
                for s, q in zip(final_state, qubits)
                if s == x
            ]

            # Apply trace
            tensor &amp;= tn.Tensor(np.reshape([1] + [0] * (2**len(inds) - 2) + [1],
                                           (2,) * len(inds)),
                                inds=inds)

        # Simplify if requested
        if kwargs[&#39;simplify_tn&#39;]:
            tensor.full_simplify_(kwargs[&#39;simplify_tn&#39;]).astype_(complex_type)
        else:
            # Otherwise, just convert to the given complex_type
            tensor.astype_(complex_type)

        # Get contraction from heuristic
        if optimize == &#39;cotengra&#39; and kwargs[&#39;max_iterations&#39;] &gt; 0:

            # Create local client if MPI has been detected (not compatible with Dask at the moment)
            if _mpi_env and kwargs[&#39;parallel&#39;]:

                from distributed import Client, LocalCluster
                _client = Client(LocalCluster(processes=False))

            else:

                _client = None

            # Set cotengra parameters
            cotengra_params = lambda: ctg.HyperOptimizer(
                methods=kwargs[&#39;methods&#39;],
                max_time=kwargs[&#39;max_time&#39;],
                max_repeats=kwargs[&#39;max_repeats&#39;],
                minimize=kwargs[&#39;minimize&#39;],
                progbar=verbose,
                parallel=kwargs[&#39;parallel&#39;],
                **kwargs[&#39;cotengra&#39;])

            # Get optimized path
            opt = cotengra_params()
            info = tensor.contract(all, optimize=opt, get=&#39;path-info&#39;)

            # Get target size
            tli = kwargs[&#39;target_largest_intermediate&#39;]

            # Repeat for the requested number of iterations
            for _ in range(1, kwargs[&#39;max_iterations&#39;]):

                # Break if largest intermediate is equal or smaller than target
                if info.largest_intermediate &lt;= tli:
                    break

                # Otherwise, restart
                _opt = cotengra_params()
                _info = tensor.contract(all, optimize=_opt, get=&#39;path-info&#39;)

                # Store the best
                if kwargs[&#39;minimize&#39;] == &#39;size&#39;:

                    if _info.largest_intermediate &lt; info.largest_intermediate or (
                            _info.largest_intermediate
                            == info.largest_intermediate and
                            _opt.best[&#39;flops&#39;] &lt; opt.best[&#39;flops&#39;]):
                        info = _info
                        opt = _opt

                else:

                    if _opt.best[&#39;flops&#39;] &lt; opt.best[&#39;flops&#39;] or (
                            _opt.best[&#39;flops&#39;] == opt.best[&#39;flops&#39;] and
                            _info.largest_intermediate &lt;
                            info.largest_intermediate):
                        info = _info
                        opt = _opt

            # Close client if exists
            if _client:

                _client.shutdown()
                _client.close()

        # Just return tensor if required
        if tensor_only:
            if optimize == &#39;cotengra&#39; and kwargs[&#39;max_iterations&#39;] &gt; 0:
                return tensor, (info, opt)
            else:
                return tensor

    else:

        # Set tensor
        tensor = circuit

        if len(optimize) == 2 and isinstance(
                optimize[0], PathInfo) and isinstance(optimize[1],
                                                      ctg.hyper.HyperOptimizer):

            # Get info and opt from optimize
            info, opt = optimize

            # Set optimization
            optimize = &#39;cotengra&#39;

        else:

            # Get tensor and path
            tensor = circuit

    # Print some info
    if verbose:
        print(
            f&#39;Largest Intermediate: 2^{np.log2(float(info.largest_intermediate)):1.2f}&#39;,
            file=stderr)
        print(
            f&#39;Max Largest Intermediate: 2^{np.log2(float(kwargs[&#34;max_largest_intermediate&#34;])):1.2f}&#39;,
            file=stderr)
        print(f&#39;Flops: 2^{np.log2(float(info.opt_cost)):1.2f}&#39;, file=stderr)

    if optimize == &#39;cotengra&#39;:

        # Get indexes
        _inds = tensor.outer_inds()

        # Get input indexes and output indexes
        _i_inds = sort([x for x in _inds if x[-2:] == &#39;_i&#39;],
                       key=lambda x: int(x.split(&#39;_&#39;)[1]))
        _f_inds = sort([x for x in _inds if x[-2:] == &#39;_f&#39;],
                       key=lambda x: int(x.split(&#39;_&#39;)[1]))

        # Get order
        _inds = [_inds.index(x) for x in _i_inds + _f_inds]

        # Get slice finder
        sf = ctg.SliceFinder(info,
                             target_size=kwargs[&#39;max_largest_intermediate&#39;])

        # Find slices
        with tqdm(kwargs[&#39;temperatures&#39;], disable=not verbose,
                  leave=False) as pbar:
            for _temp in pbar:
                pbar.set_description(f&#39;Find slices (T={_temp})&#39;)
                ix_sl, cost_sl = sf.search(temperature=_temp)

        # Get slice contractor
        sc = sf.SlicedContractor([t.data for t in tensor])

        # Update infos
        _sim_info.update({
            &#39;flops&#39;: info.opt_cost,
            &#39;largest_intermediate&#39;: info.largest_intermediate,
            &#39;n_slices&#39;: cost_sl.nslices,
            &#39;total_flops&#39;: cost_sl.total_flops
        })

        # Print some infos
        if verbose:
            print(f&#39;Number of slices: 2^{np.log2(float(cost_sl.nslices)):1.2f}&#39;,
                  file=stderr)
            print(f&#39;Flops+Cuts: 2^{np.log2(float(cost_sl.total_flops)):1.2f}&#39;,
                  file=stderr)

        if kwargs[&#39;max_n_slices&#39;] and sc.nslices &gt; kwargs[&#39;max_n_slices&#39;]:
            raise RuntimeError(
                f&#39;Too many slices ({sc.nslices} &gt; {kwargs[&#34;max_n_slices&#34;]})&#39;)

        # Contract tensor
        _li = np.log2(float(info.largest_intermediate))
        _mli = np.log2(float(kwargs[&#34;max_largest_intermediate&#34;]))
        _tensor = sc.gather_slices(
            (sc.contract_slice(i, backend=backend) for i in tqdm(
                range(sc.nslices),
                desc=f&#39;Contracting tensor (li=2^{_li:1.0f}, mli=2^{_mli:1.1f})&#39;,
                leave=False)))

        # Create map
        _map = &#39;&#39;.join([get_symbol(x) for x in range(len(_inds))])
        _map += &#39;-&gt;&#39;
        _map += &#39;&#39;.join([get_symbol(x) for x in _inds])

        # Reorder tensor
        tensor = contract(_map, _tensor)

        # Deprecated
        ## Reshape tensor
        #if _inds:
        #    if _i_inds and _f_inds:
        #        tensor = np.reshape(tensor, (2**len(_i_inds), 2**len(_f_inds)))
        #    else:
        #        tensor = np.reshape(tensor,
        #                            (2**max(len(_i_inds), len(_f_inds)),))

    else:

        # Contract tensor
        tensor = tensor.contract(optimize=optimize, backend=backend)

        if hasattr(tensor, &#39;inds&#39;):

            # Get input indexes and output indexes
            _i_inds = sort([x for x in tensor.inds if x[-2:] == &#39;_i&#39;],
                           key=lambda x: int(x.split(&#39;_&#39;)[1]))
            _f_inds = sort([x for x in tensor.inds if x[-2:] == &#39;_f&#39;],
                           key=lambda x: int(x.split(&#39;_&#39;)[1]))

            # Transpose tensor
            tensor.transpose(*(_i_inds + _f_inds), inplace=True)

            # Deprecated
            ## Reshape tensor
            #if _i_inds and _f_inds:
            #    tensor = np.reshape(tensor, (2**len(_i_inds), 2**len(_f_inds)))
            #else:
            #    tensor = np.reshape(tensor,
            #                        (2**max(len(_i_inds), len(_f_inds)),))

    if kwargs[&#39;return_info&#39;]:
        return tensor, _sim_info
    else:
        return tensor


def expectation_value(state: Array,
                      op: Circuit,
                      qubits_order: iter[any],
                      complex_type: any = &#39;complex64&#39;,
                      backend: any = &#39;numpy&#39;,
                      verbose: bool = False,
                      **kwargs) -&gt; complex:
    &#34;&#34;&#34;
    Compute expectation value of an operator given a quantum state.

    Parameters
    ----------
    state: Array
        Quantum state to use to compute the expectation value of the operator
        `op`.
    op: Circuit
        Quantum operator to use to compute the expectation value.
    qubits_order: iter[any]
        Order of qubits used to map `Circuit.qubits` to `state`.
    complex_type: any, optional
        Complex type to use to compute the expectation value.
    backend: any, optional
        Backend used to compute the quantum state. Backend must have
        `tensordot`, `transpose` and `einsum` methods.
    verbose: bool, optional
        Verbose output.

    Returns
    -------
    complex
        The expectation value of the operator `op` given `state`.

    Other Parameters
    ----------------
    `expectation_value` accepts all valid parameters for `simulate`.

    See Also
    --------
    `simulate`

    Example
    -------
    &gt;&gt;&gt; op = Circuit([
    &gt;&gt;&gt;     Gate(&#39;H&#39;, qubits=[32]),
    &gt;&gt;&gt;     Gate(&#39;CX&#39;, qubits=[32, 42]),
    &gt;&gt;&gt;     Gate(&#39;RX&#39;, qubits=[12], params=[1.32])
    &gt;&gt;&gt; ])
    &gt;&gt;&gt; expectation_value(
    &gt;&gt;&gt;     state=prepare_state(&#39;+0-&#39;),
    &gt;&gt;&gt;     op=op,
    &gt;&gt;&gt;     qubits_order=[12, 42, 32],
    &gt;&gt;&gt; )
    array(0.55860883-0.43353909j)
    &#34;&#34;&#34;

    # Fix remove_id_gates
    kwargs[&#39;remove_id_gates&#39;] = False

    # Get number of qubits
    state = np.asarray(state)

    # Get number of qubits
    n_qubits = state.ndim

    # Convert qubits_order to a list
    qubits_order = list(qubits_order)

    # Check lenght of qubits_order
    if len(qubits_order) != n_qubits:
        raise ValueError(
            &#34;&#39;qubits_order&#39; must have the same number of qubits of &#39;state&#39;.&#34;)

    # Check that qubits in op are a subset of qubits_order
    if set(op.all_qubits()).difference(qubits_order):
        raise ValueError(&#34;&#39;op&#39; has qubits not included in &#39;qubits_order&#39;.&#34;)

    # Add Gate(&#39;I&#39;) to op for not used qubits
    op = op + [
        Gate(&#39;I&#39;, qubits=[q])
        for q in set(qubits_order).difference(op.all_qubits())
    ]

    # Simulate op with given state
    _state = simulate(op,
                      initial_state=state,
                      optimize=&#39;evolution&#39;,
                      complex_type=complex_type,
                      backend=backend,
                      verbose=verbose,
                      **kwargs)

    # Return expectation value
    return np.real_if_close(np.sum(_state * state.conj()))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hybridq.circuit.simulation.simulation.expectation_value"><code class="name flex">
<span>def <span class="ident">expectation_value</span></span>(<span>state: Array, op: Circuit, qubits_order: iter[any], complex_type: any = 'complex64', backend: any = 'numpy', verbose: bool = False, **kwargs) ‑> complex</span>
</code></dt>
<dd>
<div class="desc"><p>Compute expectation value of an operator given a quantum state.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>state</code></strong> :&ensp;<code>Array</code></dt>
<dd>Quantum state to use to compute the expectation value of the operator
<code>op</code>.</dd>
<dt><strong><code>op</code></strong> :&ensp;<code>Circuit</code></dt>
<dd>Quantum operator to use to compute the expectation value.</dd>
<dt><strong><code>qubits_order</code></strong> :&ensp;<code>iter[any]</code></dt>
<dd>Order of qubits used to map <code>Circuit.qubits</code> to <code>state</code>.</dd>
<dt><strong><code>complex_type</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>Complex type to use to compute the expectation value.</dd>
<dt><strong><code>backend</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>Backend used to compute the quantum state. Backend must have
<code>tensordot</code>, <code>transpose</code> and <code>einsum</code> methods.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Verbose output.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>complex</code></dt>
<dd>The expectation value of the operator <code>op</code> given <code>state</code>.</dd>
</dl>
<h2 id="other-parameters">Other Parameters</h2>
<p><code><a title="hybridq.circuit.simulation.simulation.expectation_value" href="#hybridq.circuit.simulation.simulation.expectation_value">expectation_value()</a></code> accepts all valid parameters for <code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code>.</p>
<h2 id="see-also">See Also</h2>
<p><code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code></p>
<h2 id="example">Example</h2>
<pre><code class="language-python-repl">&gt;&gt;&gt; op = Circuit([
&gt;&gt;&gt;     Gate('H', qubits=[32]),
&gt;&gt;&gt;     Gate('CX', qubits=[32, 42]),
&gt;&gt;&gt;     Gate('RX', qubits=[12], params=[1.32])
&gt;&gt;&gt; ])
&gt;&gt;&gt; expectation_value(
&gt;&gt;&gt;     state=prepare_state('+0-'),
&gt;&gt;&gt;     op=op,
&gt;&gt;&gt;     qubits_order=[12, 42, 32],
&gt;&gt;&gt; )
array(0.55860883-0.43353909j)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def expectation_value(state: Array,
                      op: Circuit,
                      qubits_order: iter[any],
                      complex_type: any = &#39;complex64&#39;,
                      backend: any = &#39;numpy&#39;,
                      verbose: bool = False,
                      **kwargs) -&gt; complex:
    &#34;&#34;&#34;
    Compute expectation value of an operator given a quantum state.

    Parameters
    ----------
    state: Array
        Quantum state to use to compute the expectation value of the operator
        `op`.
    op: Circuit
        Quantum operator to use to compute the expectation value.
    qubits_order: iter[any]
        Order of qubits used to map `Circuit.qubits` to `state`.
    complex_type: any, optional
        Complex type to use to compute the expectation value.
    backend: any, optional
        Backend used to compute the quantum state. Backend must have
        `tensordot`, `transpose` and `einsum` methods.
    verbose: bool, optional
        Verbose output.

    Returns
    -------
    complex
        The expectation value of the operator `op` given `state`.

    Other Parameters
    ----------------
    `expectation_value` accepts all valid parameters for `simulate`.

    See Also
    --------
    `simulate`

    Example
    -------
    &gt;&gt;&gt; op = Circuit([
    &gt;&gt;&gt;     Gate(&#39;H&#39;, qubits=[32]),
    &gt;&gt;&gt;     Gate(&#39;CX&#39;, qubits=[32, 42]),
    &gt;&gt;&gt;     Gate(&#39;RX&#39;, qubits=[12], params=[1.32])
    &gt;&gt;&gt; ])
    &gt;&gt;&gt; expectation_value(
    &gt;&gt;&gt;     state=prepare_state(&#39;+0-&#39;),
    &gt;&gt;&gt;     op=op,
    &gt;&gt;&gt;     qubits_order=[12, 42, 32],
    &gt;&gt;&gt; )
    array(0.55860883-0.43353909j)
    &#34;&#34;&#34;

    # Fix remove_id_gates
    kwargs[&#39;remove_id_gates&#39;] = False

    # Get number of qubits
    state = np.asarray(state)

    # Get number of qubits
    n_qubits = state.ndim

    # Convert qubits_order to a list
    qubits_order = list(qubits_order)

    # Check lenght of qubits_order
    if len(qubits_order) != n_qubits:
        raise ValueError(
            &#34;&#39;qubits_order&#39; must have the same number of qubits of &#39;state&#39;.&#34;)

    # Check that qubits in op are a subset of qubits_order
    if set(op.all_qubits()).difference(qubits_order):
        raise ValueError(&#34;&#39;op&#39; has qubits not included in &#39;qubits_order&#39;.&#34;)

    # Add Gate(&#39;I&#39;) to op for not used qubits
    op = op + [
        Gate(&#39;I&#39;, qubits=[q])
        for q in set(qubits_order).difference(op.all_qubits())
    ]

    # Simulate op with given state
    _state = simulate(op,
                      initial_state=state,
                      optimize=&#39;evolution&#39;,
                      complex_type=complex_type,
                      backend=backend,
                      verbose=verbose,
                      **kwargs)

    # Return expectation value
    return np.real_if_close(np.sum(_state * state.conj()))</code></pre>
</details>
</dd>
<dt id="hybridq.circuit.simulation.simulation.simulate"><code class="name flex">
<span>def <span class="ident">simulate</span></span>(<span>circuit: {Circuit, TensorNetwork}, initial_state: any = None, final_state: any = None, optimize: any = 'evolution', backend: any = 'numpy', complex_type: any = 'complex64', tensor_only: bool = False, simplify: {bool, dict} = True, remove_id_gates: bool = True, use_mpi: bool = None, atol: float = 1e-08, verbose: bool = False, **kwargs) ‑> any</span>
</code></dt>
<dd>
<div class="desc"><p>Frontend to simulate <code>Circuit</code> using different optimization models and
backends.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>circuit</code></strong> :&ensp;<code>{Circuit, TensorNetwork}</code></dt>
<dd>Circuit to simulate.</dd>
<dt><strong><code>initial_state</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>Initial state to use.</dd>
<dt><strong><code>final_state</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>Final state to use (only valid for <code>optimize='tn'</code>).</dd>
<dt><strong><code>optimize</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>
<p>Optimization to use. At the moment, HybridQ supports two optimizations:
<code>optimize='evolution'</code> (equivalent to <code>optimize='evolution-hybridq'</code>)
and <code>optimize='tn'</code> (equivalent to <code>optimize='cotengra'</code>).
<code>optimize='evolution'</code> takes an <code>initial_state</code> (it can either be a
string, which is processed using <code>prepare_state</code> or an <code>Array</code>)
and evolve the quantum state accordingly to <code>Circuit</code>. Alternatives are:</p>
<ul>
<li><code>optimize='evolution-hybridq'</code>: use internal <code>C++</code> implementation for
quantum state evolution that uses vectorization instructions (such as
AVX instructions for Intel processors). This optimization method is
best suitable for <code>CPU</code>s.</li>
<li><code>optimize='evolution-einsum'</code>: use <code>einsum</code> to perform the evolution
of the quantum state (via <code>opt_einsum</code>). It is possible to futher
specify optimization for <code>opt_einsum</code> by using
<code>optimize='evolution-einsum-opt'</code> where <code>opt</code> is one of the available
optimization in <code>opt_einsum.contract</code> (default: <code>auto</code>). This
optimization is best suitable for <code>GPU</code>s and <code>TPU</code>s (using
<code>backend='jax'</code>).</li>
</ul>
<p><code>optimize='tn'</code> (or, equivalently, <code>optimize='cotengra'</code>) performs the
tensor contraction of <code>Circuit</code> given an <code>initial_state</code> and a
<code>final_state</code> (both must be a <code>str</code>). Valid tokens for both
<code>initial_state</code> and <code>final_state</code> are:</p>
<ul>
<li><code>0</code>: qubit is set to <code>0</code> in the computational basis,</li>
<li><code>1</code>: qubit is set to <code>1</code> in the computational basis,</li>
<li><code>+</code>: qubit is set to <code>+</code> state in the computational basis,</li>
<li><code>-</code>: qubit is set to <code>-</code> state in the computational basis,</li>
<li><code>.</code>: qubit is left uncontracted.</li>
</ul>
<p>Before the actual contraction, <code>cotengra</code> is called to identify an
optimal contraction. Such contraction is then used to perform the tensor
contraction.</p>
<p>If <code>Circuit</code> is a <code>TensorNetwork</code>, <code>optimize</code> must be a
valid contraction (see <code>tensor_only</code> parameter).</p>
</dd>
<dt><strong><code>backend</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>Backend used to perform the simulation. Backend must have <code>tensordot</code>,
<code>transpose</code> and <code>einsum</code> methods.</dd>
<dt><strong><code>complex_type</code></strong> :&ensp;<code>any</code>, optional</dt>
<dd>Complex type to use for the simulation.</dd>
<dt><strong><code>tensor_only</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If <code>True</code> and <code>optimize=None</code>, <code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code> will return a
<code>TensorNetwork</code> representing <code>Circuit</code>. Otherwise, if
<code>optimize='cotengra'</code>, <code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code> will return the <code>tuple</code>
<code>(TensorNetwork</code>, <code>ContractionInfo)</code>. <code>TensorNetwork</code> and
and <code>ContractionInfo</code> can be respectively used as values for
<code>circuit</code> and <code>optimize</code> to perform the actual contraction.</dd>
<dt><strong><code>simplify</code></strong> :&ensp;<code>{bool, dict}</code>, optional</dt>
<dd>Circuit is simplified before the simulation using
<code>circuit.utils.simplify</code>. If non-empty <code>dict</code> is provided, <code>simplify</code>
is passed as arguments for <code>circuit.utils.simplity</code>.</dd>
<dt><strong><code>remove_id_gates</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Identity gates are removed before to perform the simulation.
If <code>False</code>, identity gates are kept during the simulation.</dd>
<dt><strong><code>use_mpi</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use <code>MPI</code> if available. Unless <code>use_mpi=False</code>, <code>MPI</code> will be used if
detected (for instance, if <code>mpiexec</code> is used to called HybridQ). If
<code>use_mpi=True</code>, force the use of <code>MPI</code> (in case <code>MPI</code> is not
automatically detected).</dd>
<dt><strong><code>atol</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>Use <code>atol</code> as absolute tollerance.</dd>
<dt><strong><code>verbose</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Verbose output.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Output of <code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code> depends on the chosen parameters.</p>
<h2 id="other-parameters">Other Parameters</h2>
<dl>
<dt><strong><code>parallel</code></strong> :&ensp;<code>int (default: False)</code></dt>
<dd>Parallelize simulation (where possible). If <code>True</code>, the number of
available cpus is used. Otherwise, a <code>parallel</code> number of threads is
used.</dd>
<dt><strong><code>compress</code></strong> :&ensp;<code>{int, dict} (default: auto)</code></dt>
<dd>Select level of compression for <code>circuit.utils.compress</code>, which is
run on <code>Circuit</code> prior to perform the simulation. If non-empty <code>dict</code>
is provided, <code>compress</code> is passed as arguments for
<code>circuit.utils.compress</code>. If <code>optimize=evolution</code>, <code>compress</code> is set to
<code>4</code> by default. Otherwise, if <code>optimize=tn</code>, <code>compress</code> is set to <code>2</code>
by default.</dd>
<dt><strong><code>allow_sampling</code></strong> :&ensp;<code>bool (default: False)</code></dt>
<dd>If <code>True</code>, <code>Gate</code>s that provide the method <code>sample</code> will not be sampled.</dd>
<dt><strong><code>sampling_seed</code></strong> :&ensp;<code>int (default: None)</code></dt>
<dd>If provided, <code>numpy.random</code> state will be saved before sampling and
<code>sampling_seed</code> will be used to sample <code>Gate</code>s. <code>numpy.random</code> state will
be restored after sampling.</dd>
<dt><strong><code>block_until_ready</code></strong> :&ensp;<code>bool (default: True)</code></dt>
<dd>When <code>backend='jax'</code>, wait till the results are ready before returning.</dd>
<dt><strong><code>return_numpy_array</code></strong> :&ensp;<code>bool (default: True)</code></dt>
<dd>When <code>optimize='hybridq'</code> and <code>return_numpy_array</code> is <code>False, a </code>tuple`
of two <code>np.ndarray</code> is returned, corresponding to the real and
imaginary part of the quantu state. If <code>True</code>, the real and imaginary
part are copied to a single <code>np.ndarray</code> of complex numbers.</dd>
<dt><strong><code>return_info</code></strong> :&ensp;<code>bool (default: False)</code></dt>
<dd>Return extra information collected during the simulation.</dd>
<dt><strong><code>simplify_tn</code></strong> :&ensp;<code>str (default: 'RC')</code></dt>
<dd>Simplification to apply to <code>TensorNetwork</code>. Available simplifications as
specified in <code>quimb.tensor.TensorNetwork.full_simplify</code>.</dd>
<dt><strong><code>max_largest_intermediate</code></strong> :&ensp;<code>int (default: 2**26)</code></dt>
<dd>Largest intermediate which is allowed during simulation. If
<code>optimize='evolution'</code>, <code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code> will raise an error if the
largest intermediate is larger than <code>max_largest_intermediate</code>.
If
<code>optimize='tn'</code>, slicing will be applied to fit the contraction in
memory.</dd>
<dt><strong><code>target_largest_intermediate</code></strong> :&ensp;<code>int (default: 0)</code></dt>
<dd>Stop <code>cotengra</code> if a contraction having the largest intermediate smaller
than <code>target_largest_intermediate</code> is found.</dd>
<dt><strong><code>max_iterations</code></strong> :&ensp;<code>int (default: 1)</code></dt>
<dd>Number of <code>cotengra</code> iterations to find optimal contration.</dd>
<dt><strong><code>max_time</code></strong> :&ensp;<code>int (default: 120)</code></dt>
<dd>Maximum number of seconds allowed to <code>cotengra</code> to find optimal
contraction for each iteration.</dd>
<dt><strong><code>max_repeats</code></strong> :&ensp;<code>int (default: 16)</code></dt>
<dd>Number of <code>cotengra</code> steps to find optimal contraction for each
iteration.</dd>
<dt><strong><code>temperatures</code></strong> :&ensp;<code>list[float] (default: [1.0, 0.1, 0.01])</code></dt>
<dd>Temperatures used by <code>cotengra</code> to find optimal slicing of the tensor
network.</dd>
<dt><strong><code>max_n_slices</code></strong> :&ensp;<code>int (default: None)</code></dt>
<dd>If specified, <code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate()</a></code> will raise an error if the number of
slices to fit the tensor contraction in memory is larger than
<code>max_n_slices</code>.</dd>
<dt><strong><code>minimize</code></strong> :&ensp;<code>str (default: 'combo')</code></dt>
<dd>Cost function to minimize while looking for the best contraction (see
<code>cotengra</code> for more information).</dd>
<dt><strong><code>methods</code></strong> :&ensp;<code>list[str] (default: ['kahypar', 'greedy'])</code></dt>
<dd>Heuristics used by <code>cotengra</code> to find optimal contraction.</dd>
<dt><strong><code>cotengra</code></strong> :&ensp;<code>dict[any, any] (default: {})</code></dt>
<dd>Extra parameters to pass to <code>cotengra</code>.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def simulate(circuit: {Circuit, TensorNetwork},
             initial_state: any = None,
             final_state: any = None,
             optimize: any = &#39;evolution&#39;,
             backend: any = &#39;numpy&#39;,
             complex_type: any = &#39;complex64&#39;,
             tensor_only: bool = False,
             simplify: {bool, dict} = True,
             remove_id_gates: bool = True,
             use_mpi: bool = None,
             atol: float = 1e-8,
             verbose: bool = False,
             **kwargs) -&gt; any:
    &#34;&#34;&#34;
    Frontend to simulate `Circuit` using different optimization models and
    backends.

    Parameters
    ----------
    circuit: {Circuit, TensorNetwork}
        Circuit to simulate.
    initial_state: any, optional
        Initial state to use.
    final_state: any, optional
        Final state to use (only valid for `optimize=&#39;tn&#39;`).
    optimize: any, optional
        Optimization to use. At the moment, HybridQ supports two optimizations:
        `optimize=&#39;evolution&#39;` (equivalent to `optimize=&#39;evolution-hybridq&#39;`)
        and `optimize=&#39;tn&#39;` (equivalent to `optimize=&#39;cotengra&#39;`).
        `optimize=&#39;evolution&#39;` takes an `initial_state` (it can either be a
        string, which is processed using ```prepare_state``` or an `Array`)
        and evolve the quantum state accordingly to `Circuit`. Alternatives are:

        - `optimize=&#39;evolution-hybridq&#39;`: use internal `C++` implementation for
          quantum state evolution that uses vectorization instructions (such as
          AVX instructions for Intel processors). This optimization method is
          best suitable for `CPU`s.
        - `optimize=&#39;evolution-einsum&#39;`: use `einsum` to perform the evolution
          of the quantum state (via `opt_einsum`). It is possible to futher
          specify optimization for `opt_einsum` by using
          `optimize=&#39;evolution-einsum-opt&#39;` where `opt` is one of the available
          optimization in `opt_einsum.contract` (default: `auto`). This
          optimization is best suitable for `GPU`s and `TPU`s (using
          `backend=&#39;jax&#39;`).

        `optimize=&#39;tn&#39;` (or, equivalently, `optimize=&#39;cotengra&#39;`) performs the
        tensor contraction of `Circuit` given an `initial_state` and a
        `final_state` (both must be a `str`). Valid tokens for both
        `initial_state` and `final_state` are:

        - `0`: qubit is set to `0` in the computational basis,
        - `1`: qubit is set to `1` in the computational basis,
        - `+`: qubit is set to `+` state in the computational basis,
        - `-`: qubit is set to `-` state in the computational basis,
        - `.`: qubit is left uncontracted.

        Before the actual contraction, `cotengra` is called to identify an
        optimal contraction. Such contraction is then used to perform the tensor
        contraction.

        If `Circuit` is a `TensorNetwork`, `optimize` must be a
        valid contraction (see `tensor_only` parameter).
    backend: any, optional
        Backend used to perform the simulation. Backend must have `tensordot`,
        `transpose` and `einsum` methods.
    complex_type: any, optional
        Complex type to use for the simulation.
    tensor_only: bool, optional
        If `True` and `optimize=None`, `simulate` will return a
        `TensorNetwork` representing `Circuit`. Otherwise, if
        `optimize=&#39;cotengra&#39;`, `simulate` will return the `tuple`
        ```(TensorNetwork```, ```ContractionInfo)```. ```TensorNetwork``` and
        and ```ContractionInfo``` can be respectively used as values for
        `circuit` and `optimize` to perform the actual contraction.
    simplify: {bool, dict}, optional
        Circuit is simplified before the simulation using
        `circuit.utils.simplify`. If non-empty `dict` is provided, `simplify`
        is passed as arguments for `circuit.utils.simplity`.
    remove_id_gates: bool, optional
        Identity gates are removed before to perform the simulation.
        If `False`, identity gates are kept during the simulation.
    use_mpi: bool, optional
        Use `MPI` if available. Unless `use_mpi=False`, `MPI` will be used if
        detected (for instance, if `mpiexec` is used to called HybridQ). If
        `use_mpi=True`, force the use of `MPI` (in case `MPI` is not
        automatically detected).
    atol: float, optional
        Use `atol` as absolute tollerance.
    verbose: bool, optional
        Verbose output.

    Returns
    -------
    Output of `simulate` depends on the chosen parameters.

    Other Parameters
    ----------------
    parallel: int (default: False)
        Parallelize simulation (where possible). If `True`, the number of
        available cpus is used. Otherwise, a `parallel` number of threads is
        used.
    compress: {int, dict} (default: auto)
        Select level of compression for ```circuit.utils.compress```, which is
        run on `Circuit` prior to perform the simulation. If non-empty `dict`
        is provided, `compress` is passed as arguments for
        `circuit.utils.compress`. If `optimize=evolution`, `compress` is set to
        `4` by default. Otherwise, if `optimize=tn`, `compress` is set to `2`
        by default.
    allow_sampling: bool (default: False)
        If `True`, `Gate`s that provide the method `sample` will not be sampled.
    sampling_seed: int (default: None)
        If provided, `numpy.random` state will be saved before sampling and
        `sampling_seed` will be used to sample `Gate`s. `numpy.random` state will
        be restored after sampling.
    block_until_ready: bool (default: True)
        When `backend=&#39;jax&#39;`, wait till the results are ready before returning.
    return_numpy_array: bool (default: True)
        When `optimize=&#39;hybridq&#39;` and `return_numpy_array` is `False, a `tuple`
        of two `np.ndarray` is returned, corresponding to the real and
        imaginary part of the quantu state. If `True`, the real and imaginary
        part are copied to a single `np.ndarray` of complex numbers.
    return_info: bool (default: False)
        Return extra information collected during the simulation.
    simplify_tn: str (default: &#39;RC&#39;)
        Simplification to apply to `TensorNetwork`. Available simplifications as
        specified in `quimb.tensor.TensorNetwork.full_simplify`.
    max_largest_intermediate: int (default: 2**26)
        Largest intermediate which is allowed during simulation. If
        `optimize=&#39;evolution&#39;`, `simulate` will raise an error if the
        largest intermediate is larger than `max_largest_intermediate`.  If
        `optimize=&#39;tn&#39;`, slicing will be applied to fit the contraction in
        memory.
    target_largest_intermediate: int (default: 0)
        Stop `cotengra` if a contraction having the largest intermediate smaller
        than `target_largest_intermediate` is found.
    max_iterations: int (default: 1)
        Number of `cotengra` iterations to find optimal contration.
    max_time: int (default: 120)
        Maximum number of seconds allowed to `cotengra` to find optimal
        contraction for each iteration.
    max_repeats: int (default: 16)
        Number of `cotengra` steps to find optimal contraction for each
        iteration.
    temperatures: list[float] (default: [1.0, 0.1, 0.01])
        Temperatures used by `cotengra` to find optimal slicing of the tensor
        network.
    max_n_slices: int (default: None)
        If specified, `simulate` will raise an error if the number of
        slices to fit the tensor contraction in memory is larger than
        `max_n_slices`.
    minimize: str (default: &#39;combo&#39;)
        Cost function to minimize while looking for the best contraction (see
        `cotengra` for more information).
    methods: list[str] (default: [&#39;kahypar&#39;, &#39;greedy&#39;])
        Heuristics used by `cotengra` to find optimal contraction.
    cotengra: dict[any, any] (default: {})
        Extra parameters to pass to `cotengra`.
    &#34;&#34;&#34;

    # Set defaults
    kwargs.setdefault(&#39;allow_sampling&#39;, False)
    kwargs.setdefault(&#39;sampling_seed&#39;, None)

    # Convert simplify
    simplify = simplify if isinstance(simplify, bool) else dict(simplify)

    # Checks
    if tensor_only and type(optimize) == str and &#39;evolution&#39; in optimize:
        raise ValueError(
            f&#34;&#39;tensor_only&#39; is not support for optimize={optimize}&#34;)

    # Try to convert to circuit
    try:
        circuit = Circuit(circuit)
    except:
        pass

    # Simplify circuit
    if isinstance(circuit, Circuit):
        # Flatten circuit
        circuit = utils.flatten(circuit)

        # If &#39;sampling_seed&#39; is provided, use it
        if kwargs[&#39;sampling_seed&#39;] is not None:
            # Store numpy.random state
            __state = np.random.get_state()

            # Set seed
            np.random.seed(int(kwargs[&#39;sampling_seed&#39;]))

        # If stochastic gates are present, randomly sample from them
        circuit = Circuit(g.sample() if isinstance(g, pr.StochasticGate) and
                          kwargs[&#39;allow_sampling&#39;] else g for g in circuit)

        # Restore numpy.random state
        if kwargs[&#39;sampling_seed&#39;] is not None:
            np.random.set_state(__state)

        # Get qubits
        qubits = circuit.all_qubits()
        n_qubits = len(qubits)

        # Prepare state
        def _prepare_state(state):
            if isinstance(state, str):
                if len(state) == 1:
                    state *= n_qubits
                if len(state) != n_qubits:
                    raise ValueError(
                        &#34;Wrong number of qubits for initial/final state.&#34;)
                return state
            else:
                # Convert to np.ndarray
                state = np.asarray(state)
                # For now, it only supports &#34;qubits&#34; ...
                if any(x != 2 for x in state.shape):
                    raise ValueError(
                        &#34;Only qubits of dimension 2 are supported.&#34;)
                # Check number of qubits is consistent
                if state.ndim != n_qubits:
                    raise ValueError(
                        &#34;Wrong number of qubits for initial/final state.&#34;)
                return state

        # Prepare initial/final state
        initial_state = None if initial_state is None else _prepare_state(
            initial_state)
        final_state = None if final_state is None else _prepare_state(
            final_state)

        # Strip Gate(&#39;I&#39;)
        if remove_id_gates:
            circuit = Circuit(gate for gate in circuit if gate.name != &#39;I&#39;)
        # Simplify circuit
        if simplify:
            circuit = utils.simplify(
                circuit,
                remove_id_gates=remove_id_gates,
                atol=atol,
                verbose=verbose,
                **(simplify if isinstance(simplify, dict) else {}))

        # Stop if qubits have changed
        if circuit.all_qubits() != qubits:
            raise ValueError(
                &#34;Active qubits have changed after simplification. Forcing stop.&#34;
            )

    # Simulate
    if type(optimize) == str and &#39;evolution&#39; in optimize:

        # Set default parameters
        optimize = &#39;-&#39;.join(optimize.split(&#39;-&#39;)[1:])
        if not optimize:
            optimize = &#39;hybridq&#39;
        kwargs.setdefault(&#39;compress&#39;, 4)
        kwargs.setdefault(&#39;max_largest_intermediate&#39;, 2**26)
        kwargs.setdefault(&#39;return_info&#39;, False)
        kwargs.setdefault(&#39;block_until_ready&#39;, True)
        kwargs.setdefault(&#39;return_numpy_array&#39;, True)

        return _simulate_evolution(circuit, initial_state, final_state,
                                   optimize, backend, complex_type, verbose,
                                   **kwargs)
    else:

        # Set default parameters
        kwargs.setdefault(&#39;compress&#39;, 2)
        kwargs.setdefault(&#39;simplify_tn&#39;, &#39;RC&#39;)
        kwargs.setdefault(&#39;max_iterations&#39;, 1)
        try:
            import kahypar as __kahypar__
            kwargs.setdefault(&#39;methods&#39;, [&#39;kahypar&#39;, &#39;greedy&#39;])
        except ModuleNotFoundError:
            warn(&#34;Cannot find module kahypar. Remove it from defaults.&#34;)
            kwargs.setdefault(&#39;methods&#39;, [&#39;greedy&#39;])
        except ImportError:
            warn(&#34;Cannot import module kahypar. Remove it from defaults.&#34;)
            kwargs.setdefault(&#39;methods&#39;, [&#39;greedy&#39;])
        kwargs.setdefault(&#39;max_time&#39;, 120)
        kwargs.setdefault(&#39;max_repeats&#39;, 16)
        kwargs.setdefault(&#39;minimize&#39;, &#39;combo&#39;)
        kwargs.setdefault(&#39;target_largest_intermediate&#39;, 0)
        kwargs.setdefault(&#39;max_largest_intermediate&#39;, 2**26)
        kwargs.setdefault(&#39;temperatures&#39;, [1.0, 0.1, 0.01])
        kwargs.setdefault(&#39;parallel&#39;, None)
        kwargs.setdefault(&#39;cotengra&#39;, {})
        kwargs.setdefault(&#39;max_n_slices&#39;, None)
        kwargs.setdefault(&#39;return_info&#39;, False)

        # If use_mpi==False, force the non-use of MPI
        if not (use_mpi == False) and (use_mpi or _detect_mpi):

            # Warn that MPI is used because detected
            if not use_mpi:
                warn(&#34;MPI has been detected. Using MPI.&#34;)

            from hybridq.circuit.simulation.simulation_mpi import _simulate_tn_mpi
            return _simulate_tn_mpi(circuit, initial_state, final_state,
                                    optimize, backend, complex_type,
                                    tensor_only, verbose, **kwargs)
        else:

            if _detect_mpi and use_mpi == False:
                warn(
                    &#34;MPI has been detected but use_mpi==False. Not using MPI as requested.&#34;
                )

            return _simulate_tn(circuit, initial_state, final_state, optimize,
                                backend, complex_type, tensor_only, verbose,
                                **kwargs)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#types">Types</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="hybridq.circuit.simulation" href="index.html">hybridq.circuit.simulation</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hybridq.circuit.simulation.simulation.expectation_value" href="#hybridq.circuit.simulation.simulation.expectation_value">expectation_value</a></code></li>
<li><code><a title="hybridq.circuit.simulation.simulation.simulate" href="#hybridq.circuit.simulation.simulation.simulate">simulate</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>